{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\renewcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\renewcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\renewcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\renewcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\renewcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\renewcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\renewcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\renewcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\renewcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "$$\n",
    "# Part 2: Optimization and Training\n",
    "<a id=part2></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will learn how to implement optimization algorithms for deep networks. Additionally, we'll learn how to write training loops and implement a modular model trainer.\n",
    "We'll use our optimizers and training code to test a few configurations for classifying images with an MLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import unittest\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as tvtf\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "seed = 42\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "test = unittest.TestCase()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Optimization Algorithms\n",
    "<a id=part2_1></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of deep learning, an optimization algorithm is some method of iteratively updating model parameters so that the loss converges toward some local minimum (which we hope will be good enough).\n",
    "\n",
    "Gradient descent-based methods are by far the most popular algorithms for optimization of neural network parameters.\n",
    "However the high-dimensional loss-surfaces we encounter in deep learning applications are highly non-convex.\n",
    "They may be riddled with local minima, saddle points, large plateaus and a host of very challenging \"terrain\" for gradient-based optimization.\n",
    "This gave rise to many different methods of performing the parameter updates based on the loss gradients,\n",
    "aiming to tackle these optimization challenges.\n",
    "\n",
    "The most basic gradient-based update rule can be written as,\n",
    "\n",
    "$$\n",
    "\\vec{\\theta} \\leftarrow \\vec{\\theta} - \\eta \\nabla_{\\vec{\\theta}} L(\\vec{\\theta}; \\mathcal{D})\n",
    "$$\n",
    "\n",
    "where $\\mathcal{D} = \\left\\{ (\\vec{x}^i, \\vec{y}^i) \\right\\}_{i=1}^{M}$ is our training dataset or part of it. Specifically, if we have in total $N$ training samples, then\n",
    "- If $M=N$ this is known as regular gradient descent. If the dataset does not fit in memory it becomes infeasible to compute.\n",
    "- If $M=1$, the loss is computed w.r.t. a single different sample each time. This is known as stochastic gradient descent.\n",
    "- If $1<M<N$ this is known as stochastic mini-batch gradient descent. This is the most commonly-used option.\n",
    "\n",
    "The intuition behind gradient descent is simple: since the gradient of a multivariate function points to the direction of steepest ascent (\"uphill\"), we move in the opposite direction. A small step size $\\eta$ known as the **learning rate** is required since, as with any derivative, the gradient's value is only accurate close to the point it was evaluated at (the current value of $\\vec{\\theta}$).\n",
    "\n",
    "<img src=\"imgs/sgd2d.png\" width=\"600\" />\n",
    "\n",
    "The idea behind the stochastic versions is that by constantly changing the samples we compute the loss with,\n",
    "we get a dynamic error surface, i.e. it's different for each set of training samples.\n",
    "This is thought to generally improve the optimization since it may help the optimizer get out of flat regions or sharp local minima since these features may disappear in the loss surface of subsequent batches. The image below illustrates this. The different lines are different 1-dimensional losses for different training set-samples.\n",
    "\n",
    "<img src=\"imgs/sgd1d.png\" width=\"600\" />\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning frameworks generally provide implementations of various gradient-based optimization algorithms.\n",
    "Here we'll implement our own optimization module from scratch, this time keeping a similar API to the PyTorch `optim` package.\n",
    "\n",
    "We define a base `Optimizer` class. An optimizer holds a set of parameter tensors (these are the trainable parameters of some model) and maintains internal state. It may be used as follows:\n",
    "- After the forward pass has been performed the optimizer's `zero_grad()` function is invoked to clear the parameter gradients computed by previous iterations.\n",
    "- After the backward pass has been performed, and gradients have been calculated for these parameters, the optimizer's `step()` function is invoked in order to update the value of each parameter based on it's gradient.\n",
    "\n",
    "The exact method of update is implementation-specific for each optimizer and may depend on it's internal state. In addition, adding the regularization penalty to the gradient is handled by the optimizer since it only depends on the parameter values (and not the data)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the API of our `Optimizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Optimizer in module hw2.optimizers:\n",
      "\n",
      "class Optimizer(abc.ABC)\n",
      " |  Optimizer(params)\n",
      " |  \n",
      " |  Base class for optimizers.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Optimizer\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, params)\n",
      " |      :param params: A sequence of model parameters to optimize. Can be a\n",
      " |      list of (param,grad) tuples as returned by the Blocks, or a list of\n",
      " |      pytorch tensors in which case the grad will be taken from them.\n",
      " |  \n",
      " |  step(self)\n",
      " |      Updates all the registered parameter values based on their gradients.\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Sets the gradient of the optimized parameters to zero (in place).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  params\n",
      " |      :return: A sequence of parameter tuples, each tuple containing\n",
      " |      (param_data, param_grad). The data should be updated in-place\n",
      " |      according to the grad.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset({'step'})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import hw2.optimizers as optimizers\n",
    "help(optimizers.Optimizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla SGD with Regularization\n",
    "<a id=part2_2></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by implementing the simplest gradient based optimizer. The update rule will be exacly as stated above, but we'll also add a L2-regularization term to the gradient. Remember that in the **loss function**, the L2 regularization term is expressed by\n",
    "\n",
    "$$R(\\vec{\\theta}) = \\frac{1}{2}\\lambda||\\vec{\\theta}||^2_2.$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Complete the implementation of the `VanillaSGD` class in the `hw2/optimizers.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff=0.0\n"
     ]
    }
   ],
   "source": [
    "# Test VanillaSGD\n",
    "torch.manual_seed(42)\n",
    "p = torch.randn(500, 10)\n",
    "dp = torch.randn(*p.shape)*2\n",
    "params = [(p, dp)]\n",
    "\n",
    "vsgd = optimizers.VanillaSGD(params, learn_rate=0.5, reg=0.1)\n",
    "vsgd.step()\n",
    "\n",
    "expected_p = torch.load('tests/assets/expected_vsgd.pt')\n",
    "diff = torch.norm(p-expected_p).item()\n",
    "print(f'diff={diff}')\n",
    "test.assertLess(diff, 1e-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "<a id=part2_3></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can build a model and loss function, compute their gradients and we have an optimizer, we can finally do some training!\n",
    "\n",
    "In the spirit of more modular software design, we'll implement a class that will aid us in automating the repetitive training loop code that we usually write over and over again. This will be useful for both training our `Block`-based models and also later for training PyTorch `nn.Module`s.\n",
    "\n",
    "Here's our `Trainer` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Trainer in module hw2.training:\n",
      "\n",
      "class Trainer(abc.ABC)\n",
      " |  Trainer(model, loss_fn, optimizer, device=None)\n",
      " |  \n",
      " |  A class abstracting the various tasks of training models.\n",
      " |  Provides methods at multiple levels of granularity:\n",
      " |  - Multiple epochs (fit)\n",
      " |  - Single epoch (train_epoch/test_epoch)\n",
      " |  - Single batch (train_batch/test_batch)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Trainer\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, model, loss_fn, optimizer, device=None)\n",
      " |      Initialize the trainer.\n",
      " |      :param model: Instance of the model to train.\n",
      " |      :param loss_fn: The loss function to evaluate with.\n",
      " |      :param optimizer: The optimizer to train with.\n",
      " |      :param device: torch.device to run training on (CPU or GPU).\n",
      " |  \n",
      " |  fit(self, dl_train: torch.utils.data.dataloader.DataLoader, dl_test: torch.utils.data.dataloader.DataLoader, num_epochs, checkpoints: str = None, early_stopping: int = None, print_every=1, **kw) -> helpers.train_results.FitResult\n",
      " |      Trains the model for multiple epochs with a given training set,\n",
      " |      and calculates validation loss over a given validation set.\n",
      " |      :param dl_train: Dataloader for the training set.\n",
      " |      :param dl_test: Dataloader for the test set.\n",
      " |      :param num_epochs: Number of epochs to train for.\n",
      " |      :param checkpoints: Whether to save model to file every time the\n",
      " |          test set accuracy improves. Should be a string containing a\n",
      " |          filename without extension.\n",
      " |      :param early_stopping: Whether to stop training early if there is no\n",
      " |          test loss improvement for this number of epochs.\n",
      " |      :param print_every: Print progress every this number of epochs.\n",
      " |      :return: A FitResult object containing train and test losses per epoch.\n",
      " |  \n",
      " |  test_batch(self, batch) -> helpers.train_results.BatchResult\n",
      " |      Runs a single batch forward through the model and calculates loss.\n",
      " |      :param batch: A single batch of data  from a data loader (might\n",
      " |          be a tuple of data and labels or anything else depending on\n",
      " |          the underlying dataset.\n",
      " |      :return: A BatchResult containing the value of the loss function and\n",
      " |          the number of correctly classified samples in the batch.\n",
      " |  \n",
      " |  test_epoch(self, dl_test: torch.utils.data.dataloader.DataLoader, **kw) -> helpers.train_results.EpochResult\n",
      " |      Evaluate model once over a test set (single epoch).\n",
      " |      :param dl_test: DataLoader for the test set.\n",
      " |      :param kw: Keyword args supported by _foreach_batch.\n",
      " |      :return: An EpochResult for the epoch.\n",
      " |  \n",
      " |  train_batch(self, batch) -> helpers.train_results.BatchResult\n",
      " |      Runs a single batch forward through the model, calculates loss,\n",
      " |      preforms back-propagation and uses the optimizer to update weights.\n",
      " |      :param batch: A single batch of data  from a data loader (might\n",
      " |          be a tuple of data and labels or anything else depending on\n",
      " |          the underlying dataset.\n",
      " |      :return: A BatchResult containing the value of the loss function and\n",
      " |          the number of correctly classified samples in the batch.\n",
      " |  \n",
      " |  train_epoch(self, dl_train: torch.utils.data.dataloader.DataLoader, **kw) -> helpers.train_results.EpochResult\n",
      " |      Train once over a training set (single epoch).\n",
      " |      :param dl_train: DataLoader for the training set.\n",
      " |      :param kw: Keyword args supported by _foreach_batch.\n",
      " |      :return: An EpochResult for the epoch.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset({'test_batch', 'train_batch'})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import hw2.training as training\n",
    "help(training.Trainer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Trainer` class splits the task of training (and evaluating) models into three conceptual levels,\n",
    "- Multiple epochs - the `fit` method, which returns a `FitResult` containing losses and accuracies for all epochs.\n",
    "- Single epoch - the `train_epoch` and `test_epoch` methods, which return an `EpochResult` containing losses per batch and the single accuracy result of the epoch.\n",
    "- Single batch - the `train_batch` and `test_batch` methods, which return a `BatchResult` containing a single loss and the number of correctly classified samples in the batch.\n",
    "\n",
    "It implements the first two levels. Inheriting classes are expected to implement the single-batch level methods since these are model and/or task specific."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we should do in order to verify our model, gradient calculations and optimizer implementation is to try to overfit a large model (many parameters) to a small dataset (few images). This will show us that things are working properly.\n",
    "\n",
    "Let's begin by loading the CIFAR-10 dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you enconter an [SSL: CERTIFICATE_VERIFY_FAILED], uncomment the below lines and run them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ssl \n",
    "# ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train: 50000 samples\n",
      "Test: 10000 samples\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(os.getcwd(), '.pytorch-datasets')\n",
    "ds_train = torchvision.datasets.CIFAR10(root=data_dir, download=True, train=True, transform=tvtf.ToTensor())\n",
    "ds_test = torchvision.datasets.CIFAR10(root=data_dir, download=True, train=False, transform=tvtf.ToTensor())\n",
    "\n",
    "print(f'Train: {len(ds_train)} samples')\n",
    "print(f'Test: {len(ds_test)} samples')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's implement just a small part of our training logic since that's what we need right now."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**:\n",
    "1. Complete the implementation of the `train_batch()` method in the `BlocksTrainer` class within the `hw2/training.py` module.\n",
    "1. Update the hyperparameter values in the `part2_overfit_hp()` function in the `hw2/answers.py` module. Tweak the hyperparameter values until your model overfits a small number of samples in the block below. You should get 100% accuracy within a few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hw2.blocks as blocks\n",
    "import hw2.models as models\n",
    "import hw2.answers as answers\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Overfit to a very small dataset of 20 samples\n",
    "batch_size = 10\n",
    "max_batches = 2\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size, shuffle=False)\n",
    "\n",
    "# Get hyperparameters\n",
    "hp = answers.part2_overfit_hp()\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Build a model and loss using our custom MLP and CE implementations\n",
    "model = models.MLP(3*32*32, num_classes=10, hidden_features=[128]*3, wstd=hp['wstd'])\n",
    "loss_fn = blocks.CrossEntropyLoss()\n",
    "\n",
    "# Use our custom optimizer\n",
    "optimizer = optimizers.VanillaSGD(model.params(), learn_rate=hp['lr'], reg=hp['reg'])\n",
    "\n",
    "# Run training over small dataset multiple times\n",
    "trainer = training.BlocksTrainer(model, loss_fn, optimizer)\n",
    "best_acc = 0\n",
    "for i in range(20):\n",
    "    res = trainer.train_epoch(dl_train, max_batches=max_batches)\n",
    "    best_acc = res.accuracy if res.accuracy > best_acc else best_acc\n",
    "    \n",
    "test.assertGreaterEqual(best_acc, 98)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know training works, let's try to fit a model to a bit more data for a few epochs, to see how well we're doing. First, we need a function to plot the FitResults object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.plot import plot_fit\n",
    "plot_fit?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**:\n",
    "1. Complete the implementation of the `test_batch()` method in the `BlocksTrainer` class within the `hw2/training.py` module.\n",
    "1. Implement the `fit()` method of the `Trainer` class within the `hw2/training.py` module.\n",
    "1. Tweak the hyperparameters for this section in the `part2_optim_hp()` function in the `hw2/answers.py` module.\n",
    "1. Run the following blocks to train. Try to get above 35-40% test-set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a larger part of the CIFAR-10 dataset (still not the whole thing)\n",
    "batch_size = 50\n",
    "max_batches = 100\n",
    "in_features = 3*32*32\n",
    "num_classes = 10\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size, shuffle=False)\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size//2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to train a model with our Trainer and various optimizers\n",
    "def train_with_optimizer(opt_name, opt_class, fig):\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Get hyperparameters\n",
    "    hp = answers.part2_optim_hp(opt_name)\n",
    "    hidden_features = [128] * 5\n",
    "    num_epochs = 10\n",
    "    \n",
    "    # Create model, loss and optimizer instances\n",
    "    model = models.MLP(in_features, num_classes, hidden_features, wstd=hp['wstd'])\n",
    "    loss_fn = blocks.CrossEntropyLoss()\n",
    "    optimizer = opt_class(model.params(), learn_rate=hp['lr'], reg=hp['reg'])\n",
    "\n",
    "    # Train with the Trainer\n",
    "    trainer = training.BlocksTrainer(model, loss_fn, optimizer)\n",
    "    fit_res = trainer.fit(dl_train, dl_test, num_epochs, max_batches=max_batches)\n",
    "    \n",
    "    fig, axes = plot_fit(fit_res, fig=fig, legend=opt_name)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_optim = None\n",
    "fig_optim = train_with_optimizer('vanilla', optimizers.VanillaSGD, fig_optim)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum\n",
    "<a id=part2_4></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple vanilla SGD update is rarely used in practice since it's very slow to converge relative to other optimization algorithms.\n",
    "\n",
    "One reason is that naïvely updating in the direction of the current gradient causes it to fluctuate wildly in areas of the loss surface where some dimensions are much steeper than others.\n",
    "Another reason is that using the same learning rate for all parameters is not a great idea since not all parameters are created equal. For example, parameters associated with rare features should be updated with a larger step than ones associated with commonly-occurring features because they'll get less updates through the gradients.\n",
    "\n",
    "Therefore more advanced optimizers take into account the previous gradients of a parameter and/or try to use a per-parameter specific learning rate instead of a common one."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now implement a simple and common optimizer: SGD with Momentum. This optimizer takes **previous gradients** of a parameter into account when updating it's value instead of just the current one. In practice it usually provides faster convergence than the vanilla SGD.\n",
    "\n",
    "The SGD with Momentum update rule can be stated as follows:\n",
    "$$\\begin{align}\n",
    "\\vec{v}_{t+1} &= \\mu \\vec{v}_t - \\eta \\delta \\vec{\\theta}_t \\\\\n",
    "\\vec{\\theta}_{t+1} &= \\vec{\\theta}_t + \\vec{v}_{t+1}\n",
    "\\end{align}$$\n",
    "\n",
    "Where $\\eta$ is the learning rate,\n",
    "$\\vec{\\theta}$ is a model parameter,\n",
    "$\\delta \\vec{\\theta}_t=\\pderiv{L}{\\vec{\\theta}}(\\vec{\\theta}_t)$ is the gradient of the loss w.r.t. to the parameter and $\\mu$ is a hyperparameter known as momentum. \n",
    "\n",
    "Expanding the update rule recursively shows us now the parameter update infact depends on all previous gradient values for that parameter, where the old gradients are exponentially decayed by a factor of $\\mu$ at each timestep. \n",
    "\n",
    "Since we're incorporating previous gradient (update directions), a noisy value of the current gradient will have less effect so that the general direction of previous updates is maintained somewhat. The following figure illustrates this.\n",
    "\n",
    "<img src=\"imgs/sgd-momentum.png\" width=\"600\" />\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**:\n",
    "1. Complete the implementation of the `MomentumSGD` class in the `hw2/optimizers.py` module.\n",
    "1. Tweak the learning rate for momentum in `part2_optim_hp()` the function in the `hw2/answers.py` module.\n",
    "1. Run the following block to compare to the vanilla SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_optim = train_with_optimizer('momentum', optimizers.MomentumSGD, fig_optim)\n",
    "fig_optim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp\n",
    "<a id=part2_5></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is another optmizer that accounts for previous gradients, but this time it uses them to adapt the learning rate per parameter.\n",
    "\n",
    "RMSProp maintains a decaying moving average of previous squared gradients,\n",
    "$$\n",
    "\\vec{r}_{t+1} = \\gamma\\vec{r}_{t} + (1-\\gamma)\\delta\\vec{\\theta}_t^2\n",
    "$$\n",
    "\n",
    "where $0<\\gamma<1$ is a decay constant usually set close to $1$. The update rule for each parameter is then,\n",
    "$$\n",
    "\\vec{\\theta}_{t+1} = \\vec{\\theta}_t - \\left( \\frac{\\eta}{\\sqrt{r_{t+1}+\\varepsilon}} \\right) \\delta\\vec{\\theta}_t\n",
    "$$\n",
    "\n",
    "where $\\varepsilon$ is a small constant to prevent numerical instability. The idea here is to decrease the learning rate for parameters with high gradient values and vice-versa. The decaying moving average prevents accumulating all the past gradients which would cause the effective learning rate to become zero."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**:\n",
    "1. Complete the implementation of the `RMSProp` class in the `hw2/optimizers.py` module.\n",
    "1. Tweak the learning rate for RMSProp in `part2_optim_hp()` the function in the `hw2/answers.py` module.\n",
    "1. Run the following block to compare to the other optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_optim = train_with_optimizer('rmsprop', optimizers.RMSProp, fig_optim)\n",
    "fig_optim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you should get better train/test accuracy with Momentum and RMSProp than Vanilla."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization\n",
    "<a id=part2_6></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Dropout](http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf) is a useful technique to improve generalization of deep models.\n",
    "\n",
    "The idea is simple: during the forward pass drop, i.e. set to to zero, the activation of each neuron, with a probability of $p$. For example, if $p=0.5$ this means we drop half the activations on average.\n",
    "\n",
    "There are a few important things to note about dropout:\n",
    "1. It is only performed during training. When testing our model the dropout layers should be a no-op.\n",
    "1. In the backward pass, gradients are only propagated back into neurons that weren't dropped during the forward pass.\n",
    "1. During testing, the activations must be scaled since the expected value of each neuron during the training phase is now $1-p$ times it's original expectation. Thus, we need to scale the test-time activations by $1-p$ to match. Equivalently, we can scale the train time activations by $1/(1-p)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Complete the implementation of the `Dropout` class in the `hw2/blocks.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw2.grad_compare import compare_block_to_torch\n",
    "\n",
    "# Check architecture of MLP with dropout layers\n",
    "mlp_dropout = models.MLP(in_features, num_classes, [50]*3, dropout=0.6)\n",
    "print(mlp_dropout)\n",
    "test.assertEqual(len(mlp_dropout.sequence), 10)\n",
    "for b1, b2 in zip(mlp_dropout.sequence, mlp_dropout.sequence[1:]):\n",
    "    if str(b1) == 'relu':\n",
    "        test.assertTrue(str(b2).startswith('Dropout'))\n",
    "test.assertTrue(str(mlp_dropout.sequence[-1]).startswith('Linear'))\n",
    "\n",
    "# Test end-to-end gradient in train and test modes.\n",
    "mlp_dropout.train(True)\n",
    "for diff in compare_block_to_torch(mlp_dropout, torch.randn(500, in_features)):\n",
    "    test.assertLess(diff, 1e-2)\n",
    "    \n",
    "mlp_dropout.train(False)\n",
    "for diff in compare_block_to_torch(mlp_dropout, torch.randn(500, in_features)):\n",
    "    test.assertLess(diff, 1e-2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see whether dropout really improves generalization, let's take a small training set\n",
    "(small enough to overfit) and a large test set and check whether we get less overfitting and\n",
    "perhaps improved test-set accuracy when using dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a small set from CIFAR-10, but take a larger test set since we want to test generalization\n",
    "batch_size = 10\n",
    "max_batches = 40\n",
    "in_features = 3*32*32\n",
    "num_classes = 10\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size, shuffle=False)\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size*2, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**:\n",
    "Tweak the hyperparameters for this section in the `part2_dropout_hp()` function in the `hw2/answers.py` module. Try to set them so that the first model (with `dropout`=0) overfits. You can disable the other dropout options until you tune the hyperparameters. We can then see the effect of dropout for generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hyperparameters\n",
    "hp = answers.part2_dropout_hp()\n",
    "hidden_features = [400] * 1\n",
    "num_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "fig=None\n",
    "for dropout in [0, 0.4, 0.8]:\n",
    "#for dropout in [0]:\n",
    "    model = models.MLP(in_features, num_classes, hidden_features, wstd=hp['wstd'], dropout=dropout)\n",
    "    loss_fn = blocks.CrossEntropyLoss()\n",
    "    optimizer = optimizers.MomentumSGD(model.params(), learn_rate=hp['lr'], reg=0)\n",
    "\n",
    "    print('*** Training with dropout=', dropout)\n",
    "    trainer = training.BlocksTrainer(model, loss_fn, optimizer)\n",
    "    fit_res_dropout = trainer.fit(dl_train, dl_test, num_epochs, max_batches=max_batches, print_every=6)\n",
    "    fig, axes = plot_fit(fit_res_dropout, fig=fig, legend=f'dropout={dropout}', log_loss=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "<a id=part2_7></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Answer the following questions. Write your answers in the appropriate variables in the module `hw2/answers.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.answers import display_answer\n",
    "import hw2.answers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "\n",
    "Regarding the graphs you got for the three dropout configurations:\n",
    "\n",
    "1. Explain the graphs of no-dropout vs dropout. Do they match what you expected to see?\n",
    "    - If yes, explain why and provide examples based on the graphs.\n",
    "    - If no, explain what you think the problem is and what should be modified to fix it.\n",
    "\n",
    "2. Compare the low-dropout setting to the high-dropout setting and explain based on your graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_answer(hw2.answers.part2_q1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 \n",
    "\n",
    "When training a model with the cross-entropy loss function, is it possible for the test loss to **increase** for a few epochs while the test accuracy also **increases**?\n",
    "\n",
    "If it's possible explain how, if it's not explain why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_answer(hw2.answers.part2_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
